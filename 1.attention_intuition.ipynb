{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98401197-5502-4471-9814-c1d21912922b",
   "metadata": {},
   "source": [
    "# Attention intuition\n",
    "\n",
    "The lecture notes on Deep Learning from [Francois Fleuret](https://fleuret.org/dlc/materials/dlc-handout-13-2-attention-mechanisms.pdf) contain a very nice intuition on why and where the attention mechanism works better than conv nets.\n",
    "\n",
    "In the example, he considers a toy sequence-to-sequence problem with triangular and rectangular shapes with random heights as input.\n",
    "The expected target contains the same shapes but with their heights averaged, as in the figure below.\n",
    "\n",
    "\n",
    "![](images/data_example.png)\n",
    "\n",
    "Since there was no source code available in his lecture (as far as I know), I have tried to reproduce the same intuition in this notebook.\n",
    "As we can see, with the exact training procedure, the attention mechanism is able to learn the task much faster than the conv net.\n",
    "The conv net model's poor performance is expected due to its inability to look far away the input signal to learn the task.\n",
    "There are plenty of mechanisms we can equip the conv net with to make it work better (more layers, fully connected layers, ...), but the attention mechanism is a very simple and elegant solution to this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f805d4c-6f43-422d-a78c-85fad782bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with conv model\n",
      "Epoch: 0, Loss: 0.0066954898647964\n",
      "Epoch: 10, Loss: 0.006643436849117279\n",
      "Epoch: 20, Loss: 0.003941401373594999\n",
      "Epoch: 30, Loss: 0.005726605653762817\n",
      "Epoch: 40, Loss: 0.007211305201053619\n",
      "Training with attention model\n",
      "Epoch: 0, Loss: 0.008388693444430828\n",
      "Epoch: 10, Loss: 0.002573534846305847\n",
      "Epoch: 20, Loss: 0.0019854912534356117\n",
      "Epoch: 30, Loss: 0.0011634223628789186\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from shape_dataset import ShapeDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Implementation of the self attention layer\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, key_dim):\n",
    "        super().__init__()\n",
    "        self.conv_Q = nn.Conv1d(in_dim, key_dim, kernel_size=1, bias=False)\n",
    "        self.conv_K = nn.Conv1d(in_dim, key_dim, kernel_size=1, bias=False)\n",
    "        self.conv_V = nn.Conv1d(in_dim, out_dim, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.conv_Q(x)\n",
    "        K = self.conv_K(x)\n",
    "        V = self.conv_V(x)\n",
    "        A = Q.transpose(1, 2).matmul(K).softmax(2)\n",
    "        y = A.matmul(V.transpose(1, 2)).transpose(1, 2)\n",
    "        return y\n",
    "    \n",
    "def train_model(model, epochs=20, device=\"cuda\"):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=ShapeDataset(size=100, max_height=50, noise_std=0.3, max_samples=10000),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_per_epoch = []    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        loss_per_epoch.append(loss.item())\n",
    "\n",
    "    return loss_per_epoch\n",
    "\n",
    "\n",
    "# Training the regular model with no self attention\n",
    "print(\"Training with conv model\")\n",
    "conv_model = nn.Sequential(\n",
    "    nn.Conv1d(1, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 1, kernel_size=(5,), padding=\"same\"),\n",
    ")\n",
    "loss_per_epoch_conv = train_model(conv_model, epochs=50)\n",
    "\n",
    "# Training with self attention\n",
    "print(\"Training with attention model\")\n",
    "attention_model = nn.Sequential(\n",
    "    nn.Conv1d(1, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    SelfAttentionLayer(in_dim=64, out_dim=64, key_dim=64),\n",
    "    nn.Conv1d(64, 64, kernel_size=(5,), padding=\"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(64, 1, kernel_size=(5,), padding=\"same\"),\n",
    ")\n",
    "loss_per_epoch_attention = train_model(attention_model, epochs=50)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_per_epoch_conv, label=\"Without Attention\")\n",
    "plt.plot(loss_per_epoch_attention, label=\"With Attention\")\n",
    "plt.legend()\n",
    "\n",
    "pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268a970-c5d4-4cf2-8102-a74ab02e5e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
